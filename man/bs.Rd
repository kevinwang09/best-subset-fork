% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bs.R
\name{bs}
\alias{bs}
\title{Best subset selection.}
\usage{
bs(x, y, k = 0:min(nrow(x) - intercept, ncol(x), 200), intercept = TRUE,
  form = ifelse(nrow(x) < ncol(x), 2, 1), time.limit = 100, nruns = 50,
  maxiter = 1000, tol = 1e-04, polish = TRUE, verbose = FALSE)
}
\arguments{
\item{x}{Matrix of predictors, of dimension (say) n x p.}

\item{y}{Vector of responses, of length (say) n.}

\item{k}{Sparsity level, i.e., number of nonzero coefficients to allow in the
subset regression model; can be a vector, in which case the best subset
selection problem is solved for every value of the sparsity level. Default
is 0:min(n-1,p,200) for models with intercept and 0:min(n,p,200) for models
without it.}

\item{intercept}{Should an intercept be included in the regression model?
Default is TRUE.}

\item{form}{Either of 1 or 2, specifying the formulation to use for best
subset solution as a mixed integer quadratic program. Formulations 1 and 2
correspond to equations (2.5) and (2.6) in Bertsimas, King, and Mazumder
(2016), see references below. Default is 1 if n >= p, and 2 if n < p.}

\item{time.limit}{The maximum amount of time (in seconds) to allow Gurobi to
compute the subset selection solution at each value of k. Default is 100.}

\item{nruns}{The number of runs of projected gradient descent to use, where
each run begins at a random initialization for the coefficients. The best
estimate over all these runs (achieving the lowest criterion value) is
passed to Gurobi as a warm start. Default is 50.}

\item{maxiter}{The maximum number of iterations for each run of projected
gradient descent. Default is 1000.}

\item{tol}{The tolerance for the relative difference in coefficients between
iterations of projected gradient descent (the algorithm terminates when the
relative difference is less than the specified tolerance). Default is 1e-4.}

\item{polish}{Should the project gradient descent algorithm replace the
estimate at each iteration by the least squares solution on the active set?
Default is TRUE.}

\item{Should}{intermediate progress be printed out? Default is FALSE.}
}
\value{
A list with the following components:
  \itemize{
  \item beta: matrix of regression coefficients, one column per sparsity
    level
  \item status: vector of status strings returned by Gurobi's MIO solver,
    one element for each sparsity level
  \item k: vector of sparsity levels
  \item x, y: the passed x and y
  \item bx, by: the means of the columns of x, and the mean of y
  \item intercept: was an intercept included?
  }
}
\description{
Compute best subset selection solutions.
}
\details{
This function solves best subset selection program:
  \deqn{\min_\beta \|Y - X \beta\|_2^2 \;\;{\rm s.t.}\;\; \|\beta\|_0 \leq k}
  for a response vector \eqn{Y} and predictor matrix \eqn{X}. It follows the
  algorithm suggested by Bertismas, King, and Mazumder (2016) (see below for
  for the full reference): it first uses projected gradient descent to find
  an approximate solution to the above nonconvex program, and then calls
  Gurobi's MIO (mixed integer optimization) solver with this approximate
  solution as a warm start.
  There are two options for how to formulate best subset selection as a mixed
  integer quadratic program, one recommended when n >= p, and the other when
  n < p. They correspond to equations (2.5) and (2.6) in the paper by
  Bertismas, King, and Mazumder (2016), respectively.
}
\examples{
# Simulate some simple regression data with the first 5 coefficients
# being nonzero
set.seed(0)
n = 100
p = 20
xy.obj = sim.xy(n,p,nval=0,s=5,beta.type=2,snr=1)
x = xy.obj$x
y = xy.obj$y

# Run forward stepwise regression for 8 steps
fs.obj = fs(x,y,intercept=FALSE,maxsteps=8,verbose=TRUE)
fs.beta = coef(fs.obj)
fs.supp = apply(fs.beta != 0, 2, which)

# Solve best subset selection for 8 sparsity levels
bs.obj = bs(x,y,intercept=FALSE,k=0:8,verbose=TRUE)
bs.beta = coef(bs.obj)
bs.supp = apply(bs.beta != 0, 2, which)

# Compare supports of the solutions with 5 and 8 variables
fs.supp[[6]]; bs.supp[[6]]
fs.supp[[9]]; bs.supp[[9]]

# Predict on test data and record test error
ntest = 10000
xy.obj.test = sim.xy(ntest,p,nval=0,s=5,beta.type=2,snr=1)
xtest = xy.obj.test$x
ytest = xy.obj.test$y

fs.pred = predict(fs.obj,newx=xtest)
bs.pred = predict(bs.obj,newx=xtest)
colMeans((fs.pred - ytest)^2)
colMeans((bs.pred - ytest)^2)
}
\author{
Ryan Tibshirani
}
\references{
This function utilizes the MIO formulation for subset selection
  as described in "Best subset selection via a modern optimization lens" by
  Dimitris Bertsimas, Angela King, and Rahul Mazumder, Annals of Statistics,
  44(2), 813-852, 2016. This R implementation is based on Matlab code written
  by Rahul Mazumder.
}

